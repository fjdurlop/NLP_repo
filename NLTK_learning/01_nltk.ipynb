{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8ead55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1b0bd",
   "metadata": {},
   "source": [
    "- tokenizing - word tokenizers  sentences tokenizers\n",
    "- lexicon and corporas\n",
    "- corpora or corpus - body of text. (Medical journals, presidential speeches, english language)\n",
    "- Lexicon - words and their meaning\n",
    "\n",
    "\n",
    "\n",
    "- Investor-speak .... regular english-speak\n",
    "\n",
    "- investor-speak 'bull'  = someone who is positive about the market\n",
    "- english-speak 'bull' = scary animal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea104d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b02da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35eac70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c2e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78584bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a638cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith, how are you doing today?',\n",
       " 'The weather is great, and Python is awesome.',\n",
       " 'The sky is pinkish-blue.',\n",
       " \"You shouldn't eat cardboard.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2972ef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c7f9d0",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Useless words of the language, words that contain no meaning, to reduce space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44578293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c29847b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) #can add stop words\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c55e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef1d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ce2b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b074fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c6705",
   "metadata": {},
   "source": [
    "### Stop words Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53285420",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_esp = \"Bueno, tenemos una reunión para la semana próxima sobre el Gas Bienestar y yo puedo informarles la semana que viene de cómo vamos. Ya ha avanzado en la creación de la empresa. Como lo prometimos, vamos a iniciar en tres meses, yo creo que antes, considerando el tiempo que ya transcurrió desde que lo dimos a conocer. Vamos a pensar que en dos meses más ya estamos distribuyendo el Gas Bienestar en la Ciudad de México, vamos a empezar aquí y poco a poco se va a ir ampliando; pero ya se elaboró el programa, ya se tienen los terrenos para las centrales de distribución, ya se están adquiriendo los cilindros, las camionetas repartidoras, ya estamos avanzando.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef203043",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('spanish')) #can add stop words\n",
    "\n",
    "word_tokens = word_tokenize(ex_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b102a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(ex_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5c30344",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "#filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aae626ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original sentence,  660\n",
      "Length of filtered sentence,  67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bueno',\n",
       " ',',\n",
       " 'reunión',\n",
       " 'semana',\n",
       " 'próxima',\n",
       " 'Gas',\n",
       " 'Bienestar',\n",
       " 'puedo',\n",
       " 'informarles',\n",
       " 'semana',\n",
       " 'viene',\n",
       " 'cómo',\n",
       " 'vamos',\n",
       " '.',\n",
       " 'Ya',\n",
       " 'avanzado',\n",
       " 'creación',\n",
       " 'empresa',\n",
       " '.',\n",
       " 'Como',\n",
       " 'prometimos',\n",
       " ',',\n",
       " 'vamos',\n",
       " 'iniciar',\n",
       " 'tres',\n",
       " 'meses',\n",
       " ',',\n",
       " 'creo',\n",
       " ',',\n",
       " 'considerando',\n",
       " 'tiempo',\n",
       " 'transcurrió',\n",
       " 'dimos',\n",
       " 'conocer',\n",
       " '.',\n",
       " 'Vamos',\n",
       " 'pensar',\n",
       " 'dos',\n",
       " 'meses',\n",
       " 'distribuyendo',\n",
       " 'Gas',\n",
       " 'Bienestar',\n",
       " 'Ciudad',\n",
       " 'México',\n",
       " ',',\n",
       " 'vamos',\n",
       " 'empezar',\n",
       " 'aquí',\n",
       " 'va',\n",
       " 'ir',\n",
       " 'ampliando',\n",
       " ';',\n",
       " 'elaboró',\n",
       " 'programa',\n",
       " ',',\n",
       " 'terrenos',\n",
       " 'centrales',\n",
       " 'distribución',\n",
       " ',',\n",
       " 'adquiriendo',\n",
       " 'cilindros',\n",
       " ',',\n",
       " 'camionetas',\n",
       " 'repartidoras',\n",
       " ',',\n",
       " 'avanzando',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(\"Length of original sentence, \",len(ex_esp))\n",
    "print(\"Length of filtered sentence, \",len(filtered_sentence))\n",
    "\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe8a3a",
   "metadata": {},
   "source": [
    "## Stemming words (palabras derivadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fad21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d15bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f389183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c4ea3be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stem() missing 1 required positional argument: 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d46f1fe1ed3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexample_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: stem() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91bdb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfdead07",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stem() missing 1 required positional argument: 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-421579b92880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: stem() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1bbbaa",
   "metadata": {},
   "source": [
    "## part of speech tagging\n",
    "This means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more. Here's a list of the tags, what they mean, and some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "853e4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer # unsupervised tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d901c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's create our training and testing data\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18731491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0376eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95dd7a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e119db6",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39feaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "##process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15c3fc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31005"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6072172",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ada0c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "\n",
    "\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd5cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
